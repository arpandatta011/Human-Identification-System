{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Human-Identification-System\\deep_person_reid\\torchreid\\metrics\\rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Necessary Imports\n",
    "import os, requests, torch, math, cv2, sys, PIL, argparse,imutils, time, json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from typing import List, Optional\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "\n",
    "# Imports for YoloV6 for Object Detection\n",
    "from YOLOv6 import yolov6\n",
    "sys.modules['yolov6'] = yolov6\n",
    "\n",
    "from YOLOv6.yolov6.utils.events import LOGGER, load_yaml\n",
    "from YOLOv6.yolov6.layers.common import DetectBackend\n",
    "from YOLOv6.yolov6.data.data_augment import letterbox\n",
    "from YOLOv6.yolov6.utils.nms import non_max_suppression\n",
    "from YOLOv6.yolov6.core.inferer import Inferer\n",
    "\n",
    "# Imports for Object Reidentification \n",
    "from deep_person_reid import torchreid\n",
    "sys.modules['torchreid'] = torchreid\n",
    "from deep_person_reid.torchreid.utils import FeatureExtractor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if cuda else 'cpu')\n",
    "\n",
    "\n",
    "class_names = [ 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "         'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "         'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "         'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "         'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "         'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "         'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "         'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "         'hair drier', 'toothbrush' ]\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_img_size(img_size, s=32, floor=0):\n",
    "    def make_divisible( x, divisor):\n",
    "      # Upward revision the value x to make it evenly divisible by the divisor.\n",
    "      return math.ceil(x / divisor) * divisor\n",
    "    \"\"\"Make sure image size is a multiple of stride s in each dimension, and return a new shape list of image.\"\"\"\n",
    "    if isinstance(img_size, int):  # integer i.e. img_size=640\n",
    "        new_size = max(make_divisible(img_size, int(s)), floor)\n",
    "    elif isinstance(img_size, list):  # list i.e. img_size=[640, 480]\n",
    "        new_size = [max(make_divisible(x, int(s)), floor) for x in img_size]\n",
    "    else:\n",
    "        raise Exception(f\"Unsupported type of img_size: {type(img_size)}\")\n",
    "\n",
    "    if new_size != img_size:\n",
    "        print(f'WARNING: --img-size {img_size} must be multiple of max stride {s}, updating to {new_size}')\n",
    "    return new_size if isinstance(img_size,list) else [new_size]*2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def precess_image(path, img_size, stride):\n",
    "    \n",
    "    '''Process image before image inference.'''\n",
    "        \n",
    "    try:\n",
    "        from PIL import Image\n",
    "        if type(path) == str:\n",
    "            img_src = np.asarray(Image.open(path))\n",
    "        else:\n",
    "            img_src = path\n",
    "        assert img_src is not None, f'Invalid image: {path}'\n",
    "        \n",
    "    except Exception as e:\n",
    "        LOGGER.Warning(e)\n",
    "        \n",
    "    image = letterbox(img_src, img_size, stride=stride)[0]\n",
    "\n",
    "    # Convert\n",
    "    image = image.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "    image = torch.from_numpy(np.ascontiguousarray(image))\n",
    "    image = image.float()  # uint8 to fp16/32\n",
    "    image /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "\n",
    "    return image, img_src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from models/yolov6l.pt\n",
      "\n",
      "Fusing model...\n",
      "d:\\python\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "model = DetectBackend(f\"models/yolov6l.pt\", device=device)\n",
    "stride = model.stride\n",
    "class_names = load_yaml(\"YOLOv6/data/coco.yaml\")['names']\n",
    "\n",
    "model.model.float()\n",
    "\n",
    "img_size = (640,640)\n",
    "if device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, *img_size).to(device).type_as(next(model.model.parameters())))  # warmup\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(image):\n",
    "\n",
    "    hide_labels: bool = False \n",
    "    hide_conf: bool = False \n",
    "\n",
    "    img_size:int = 640\n",
    "\n",
    "    conf_thres: float =.25\n",
    "    iou_thres: float =.45\n",
    "    max_det:int =  1000\n",
    "    agnostic_nms: bool = False \n",
    "\n",
    "    img_size = check_img_size(img_size, s=stride)\n",
    "\n",
    "    img, img_src = precess_image(image, img_size, stride)\n",
    "    img = img.to(device)\n",
    "    if len(img.shape) == 3:\n",
    "        img = img[None]\n",
    "        # expand for batch dim\n",
    "    pred_results = model(img)\n",
    "    classes:Optional[List[int]] = None # the classes to keep\n",
    "    det = non_max_suppression(pred_results, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)[0]\n",
    "\n",
    "    gn = torch.tensor(img_src.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "    img_ori = img_src.copy()\n",
    "    if len(det):\n",
    "        det[:, :4] = Inferer.rescale(img.shape[2:], det[:, :4], img_src.shape).round()\n",
    "    return det\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_text(img, text,\n",
    "          pos=(0, 0),\n",
    "          font=cv2.FONT_HERSHEY_PLAIN,\n",
    "          font_scale=3,\n",
    "          text_color=(0, 255, 0),\n",
    "          font_thickness=2,\n",
    "          text_color_bg=(0, 0, 0)\n",
    "          ):\n",
    "\n",
    "    x, y = pos\n",
    "    font_scale = 1\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    text_size, _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
    "    text_w, text_h = text_size\n",
    "    cv2.rectangle(img, (x, y - text_h - 10), (x + text_w + 10, y), text_color_bg, -1)\n",
    "    cv2.putText(img, text, (x+5, y-5), font, font_scale, text_color, font_thickness)\n",
    "\n",
    "\n",
    "def draw_bb_text(frame, text,\n",
    "          bbox,\n",
    "          font=cv2.FONT_HERSHEY_PLAIN,\n",
    "          font_scale=3,\n",
    "          text_color=(0, 255, 0),\n",
    "          font_thickness=2,\n",
    "          color_bg=(255, 255, 255)\n",
    "          ):\n",
    "\n",
    "    startX, startY, endX, endY = bbox\n",
    "    text_size, _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
    "    text_w, text_h = text_size\n",
    "    startY = 20 if startY < 20 else startY\n",
    "    startX = 1 if startX < 1 else startX\n",
    "    bg = np.ones_like(frame[startY-20:startY,startX-1:startX+text_w+3]).astype('uint8') * 255\n",
    "    bg[:,:] = color_bg\n",
    "    frame[startY-20:startY,startX-1:startX+text_w+3] = cv2.addWeighted(frame[startY-20:startY,startX-1:startX+text_w+3], 0.0, bg, 1.0, 1)\n",
    "    \n",
    "    cv2.rectangle(frame, (startX, startY), (endX, endY), color_bg, 2)\n",
    "    cv2.putText(frame, text, (startX, startY-text_h+2), font, font_scale, text_color, font_thickness)\n",
    "\n",
    "    \n",
    "def prepare_cropped_images(img):\n",
    "    \n",
    "    if type(img) == str:\n",
    "        img = np.array(Image.open(img))\n",
    "\n",
    "    if img.shape[0] > img.shape[1]:\n",
    "        img = imutils.resize(img, height=500)\n",
    "    else:\n",
    "        img = imutils.resize(img, width=500)\n",
    "\n",
    "    h, w, c = img.shape\n",
    "\n",
    "    ones = np.ones([h, 500, 3]).astype('uint8')\n",
    "    ones[np.all(True)] = (0,0,0)\n",
    "    ones[:, 250 - (w // 2) : 250 - (w // 2) + w] = img\n",
    "    img = ones\n",
    "\n",
    "    text_area = np.ones([30, 500, 3]).astype('uint8')\n",
    "    text_area[np.all(True)] = (255,150,100)\n",
    "\n",
    "    h, w, c = img.shape\n",
    "\n",
    "    final = np.ones([500, 500, 3]).astype('uint8') * 255\n",
    "\n",
    "    final[250 - (h // 2) : 250 - (h // 2) + h, 250 - (w // 2) : 250 - (w // 2) + w] = img\n",
    "\n",
    "    return final\n",
    "\n",
    "\n",
    "def get_color(idx):\n",
    "    idx = idx * 3\n",
    "    color = (int((37 * idx) % 255), int((17 * idx) % 255), int((29 * idx) % 255))\n",
    "\n",
    "    return color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: osnet_ain_x1_0\n",
      "- params: 2,193,616\n",
      "- flops: 978,878,352\n",
      "Successfully loaded pretrained weights from \"models/osnet_ain.pth.tar-50\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n"
     ]
    }
   ],
   "source": [
    "extractor = FeatureExtractor(\n",
    "    model_name='osnet_ain_x1_0',\n",
    "    device=\"cpu\" if device == torch.device(\"cpu\") else \"cuda\",\n",
    "    model_path = 'models/osnet_ain.pth.tar-50'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_feature_extractor(img, vis = False):\n",
    "    \n",
    "    if type(img) == str:\n",
    "        img = np.array(Image.open(img))\n",
    "        \n",
    "    if vis:\n",
    "        \n",
    "        if img.shape[0] > img.shape[1]:\n",
    "            img = imutils.resize(img, height=1000)\n",
    "        else:\n",
    "            img = imutils.resize(img, width=1000)\n",
    "        \n",
    "    dets = detect(img).cpu().numpy()\n",
    "    \n",
    "    det_features_list = []\n",
    "    extracted_humans = []\n",
    "    \n",
    "    if vis:\n",
    "        cropped_images = []\n",
    "        img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        i = 1\n",
    "    \n",
    "    for det in dets:\n",
    "        x1, y1, x2, y2, conf, cls = det\n",
    "        startX, startY, endX, endY = int(x1), int(y1), int(x2), int(y2)\n",
    "        cropped = img[startY:endY,startX:endX]\n",
    "        extracted_humans.append(cropped)\n",
    "        \n",
    "        if vis:\n",
    "            cropped_images.append(prepare_cropped_images(img_bgr[startY:endY,startX:endX]))\n",
    "            cv2.rectangle(img_bgr, (startX, startY), (endX, endY), get_color(i), 2)\n",
    "            i+=1\n",
    "         \n",
    "    if vis:\n",
    "        cv2.imshow('detections', img_bgr)\n",
    "        humans_n = len(cropped_images)\n",
    "        root_n = humans_n**0.5\n",
    "\n",
    "        if root_n % 1 != 0:\n",
    "            root_n = int(root_n) + 1\n",
    "\n",
    "        root_n = int(root_n)\n",
    "\n",
    "        cropped_images = np.array(cropped_images + [x for x in np.ones((root_n ** 2 - humans_n,) + cropped_images[0].shape)]).reshape(root_n, root_n, 500, 500, 3).astype('uint8')\n",
    "\n",
    "        \n",
    "        humans_grid = np.vstack([np.hstack(x) for x in cropped_images])\n",
    "        \n",
    "        if humans_grid.shape[0] > humans_grid.shape[1]:\n",
    "            humans_grid = imutils.resize(humans_grid, height=750)\n",
    "        else:\n",
    "            humans_grid = imutils.resize(humans_grid, width=750)\n",
    "            \n",
    "        cv2.imshow('extracted humans', humans_grid)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "    return dets, extractor(extracted_humans) if len(extracted_humans) != 0 else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.34 s\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = human_feature_extractor('marine-drive.jpg', vis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_known_humans(path, is_stream = False, threshold = 0.6):\n",
    "    \n",
    "    if is_stream:\n",
    "        stream = VideoStream(path).start()\n",
    "    else:\n",
    "        video = cv2.VideoCapture(path)\n",
    "        video_fps = video.get(cv2.CAP_PROP_FPS)\n",
    "        \n",
    "    target_list = []\n",
    "    gallery = {}\n",
    "    queries = None\n",
    "    \n",
    "    key = None\n",
    "    processing_speed = 1\n",
    "    \n",
    "    fps = FPS().start() \n",
    "    \n",
    "    frame_no = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        if is_stream:\n",
    "            frame = stream.read()\n",
    "        else:\n",
    "            _, frame = video.read()\n",
    "            \n",
    "        if frame is None:\n",
    "            break\n",
    "            \n",
    "        frame_no += 1\n",
    "            \n",
    "        if frame_no % processing_speed != 0 and not is_stream:\n",
    "            continue\n",
    "            \n",
    "        if frame.shape[0] > frame.shape[1]:\n",
    "            frame = imutils.resize(frame, height=750)\n",
    "        else:\n",
    "            frame = imutils.resize(frame, width=750)\n",
    "        \n",
    "\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if key == ord('s'):\n",
    "\n",
    "            initBB = cv2.selectROI('Please select the targets - ', frame)\n",
    "            cv2.destroyWindow('Please select the targets - ')\n",
    "            target_name = input(\"Enter target name : \")\n",
    "            \n",
    "            if target_name not in target_list:\n",
    "                target_list.append(target_name)\n",
    "            \n",
    "            (startX, startY, endX, endY) = initBB[0], initBB[1], initBB[0] + initBB[2], initBB[1] + initBB[3]\n",
    "            croped = rgb[startY:endY,startX:endX]            \n",
    "\n",
    "            gallery[target_name] = extractor(croped)\n",
    "            \n",
    "            fps = FPS().start() \n",
    "            \n",
    "            \n",
    "        if len(target_list) != 0:\n",
    "            dets, queries = human_feature_extractor(rgb)\n",
    "            \n",
    "        if queries is not None:\n",
    "            if len(queries) == 0:\n",
    "                continue\n",
    "        \n",
    "        for idx,target_name in enumerate(target_list):\n",
    "            \n",
    "            res = torchreid.metrics.compute_distance_matrix(gallery[target_name], queries,'cosine')\n",
    "            \n",
    "            sim = 1-res[0].cpu().numpy()\n",
    "        \n",
    "            max_sim_score, max_sim_id = np.max(sim), np.argmax(sim)\n",
    "            \n",
    "            if max_sim_score > threshold:\n",
    "                \n",
    "                x1, y1, x2, y2, conf, cls = dets[max_sim_id]\n",
    "                startX, startY, endX, endY = int(x1), int(y1), int(x2), int(y2)\n",
    "                \n",
    "                draw_bb_text(frame, f' {target_name}, sim : {str(round(max_sim_score,2))}', (startX, startY, endX, endY),cv2.FONT_HERSHEY_DUPLEX, 0.4, (0, 0, 0), 1, get_color(idx+1))\n",
    "\n",
    "                \n",
    "        \n",
    "        fps.update()\n",
    "        fps.stop()\n",
    "        \n",
    "        draw_text(frame, \"FPS : {:.2f}\".format(fps.fps()), (1, 20),cv2.FONT_HERSHEY_DUPLEX, 0.6, (50, 50, 50), 2, (200, 200, 200))\n",
    "        \n",
    "        \n",
    "        if fps.fps() < 1 and not is_stream:\n",
    "            processing_speed = int(5 / fps.fps())\n",
    "        \n",
    "        cv2.imshow('Display', frame)\n",
    "        \n",
    "        if is_stream:\n",
    "            key = cv2.waitKey(1)\n",
    "        else:\n",
    "            key = cv2.waitKey(int(video_fps))\n",
    "            \n",
    "        if key == ord('q'):\n",
    "            \n",
    "            break\n",
    "            \n",
    "            \n",
    "    if is_stream:\n",
    "        stream.stop()\n",
    "    else:\n",
    "        video.release()\n",
    "    \n",
    "    cv2.destroyWindow('Display')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_known_humans(path = 'demo_videos/ipcam1.mp4', is_stream = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_known_humans(path = 0, is_stream = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Person Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multi_person_tracking(path, is_stream = False):\n",
    "    \n",
    "    \n",
    "    if is_stream:\n",
    "        stream = VideoStream(path).start()\n",
    "    else:\n",
    "        video = cv2.VideoCapture(path)\n",
    "        video_fps = video.get(cv2.CAP_PROP_FPS)\n",
    "        \n",
    "    track_targets = []\n",
    "    \n",
    "    key = None\n",
    "    processing_speed = 1\n",
    "    \n",
    "    fps = FPS().start() \n",
    "    \n",
    "    frame_no = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        if is_stream:\n",
    "            frame = stream.read()\n",
    "        else:\n",
    "            _, frame = video.read()\n",
    "            \n",
    "        if frame is None:\n",
    "            break\n",
    "            \n",
    "        \n",
    "            \n",
    "        if frame_no % processing_speed != 0 and not is_stream:\n",
    "            frame_no += 1\n",
    "            continue\n",
    "            \n",
    "        if frame.shape[0] > frame.shape[1]:\n",
    "            frame = imutils.resize(frame, height=750)\n",
    "        else:\n",
    "            frame = imutils.resize(frame, width=750)\n",
    "            \n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        \n",
    "        dets = detect(rgb).cpu().detach().numpy()\n",
    "\n",
    "        dets_filtered = []    \n",
    "        croped_images = []\n",
    "\n",
    "        for det in dets:\n",
    "            x1, y1, x2, y2, conf, cls = det\n",
    "            startX, startY, endX, endY = int(x1), int(y1), int(x2), int(y2)\n",
    "            class_name = class_names[int(cls)]\n",
    "            if conf > 0.45 and class_name in ['person']:\n",
    "                dets_filtered.append([startX, startY, endX, endY, class_name, round(conf, 2)])\n",
    "                croped = rgb[startY:endY,startX:endX]\n",
    "                croped_images.append(croped)\n",
    "\n",
    "\n",
    "        if len(croped_images) != 0:\n",
    "\n",
    "            features_curr = extractor(croped_images)\n",
    "\n",
    "\n",
    "            if frame_no == 0 or track_targets == []:\n",
    "\n",
    "                track_targets = features_curr\n",
    "\n",
    "            else:\n",
    "\n",
    "                sim_res = 1-torchreid.metrics.compute_distance_matrix(features_curr,track_targets,'cosine').cpu().detach().numpy()\n",
    "\n",
    "                query_id_used, gallery_id_used = [], []\n",
    "                for sim in -np.sort(-np.concatenate(sim_res)):\n",
    "                    sim_ids = np.array(np.where(sim_res == sim)).T\n",
    "                    for query_id, gallery_id in sim_ids:\n",
    "                        startX, startY, endX, endY, class_name, conf = dets_filtered[query_id]\n",
    "                        if query_id not in query_id_used and gallery_id not in gallery_id_used and sim > 0.60:\n",
    "                            cv2.rectangle(frame, (startX, startY), (endX, endY),get_color(idx=gallery_id+1), 2)\n",
    "                            draw_bb_text(frame, f'{class_name}, ID : {gallery_id}', (startX, startY, endX, endY),cv2.FONT_HERSHEY_DUPLEX, 0.4, (0, 0, 0), 1, get_color(idx=gallery_id+1))\n",
    "\n",
    "\n",
    "                            if sim > 0.65:\n",
    "                                track_targets[gallery_id] = features_curr[query_id]\n",
    "\n",
    "                            query_id_used.append(query_id), gallery_id_used.append(gallery_id)\n",
    "\n",
    "                        elif query_id not in query_id_used and sim_res[query_id].max() < 0.55:\n",
    "                            track_targets = torch.cat([track_targets, torch.tensor([features_curr[query_id].tolist()]).to(device)])\n",
    "                            query_id_used.append(query_id)\n",
    "\n",
    "        \n",
    "        fps.update()\n",
    "        fps.stop()\n",
    "        frame_no += 1\n",
    "        \n",
    "        draw_text(frame, \"FPS : {:.2f}\".format(fps.fps()), (1, 20),cv2.FONT_HERSHEY_DUPLEX, 0.6, (50, 50, 50), 2, (200, 200, 200))\n",
    "        \n",
    "        \n",
    "        if fps.fps() < 1 and not is_stream:\n",
    "            processing_speed = int(5 / fps.fps())\n",
    "        \n",
    "        cv2.imshow('Display', frame)\n",
    "        \n",
    "        if is_stream:\n",
    "            key = cv2.waitKey(1)\n",
    "        else:\n",
    "            key = cv2.waitKey(int(video_fps))\n",
    "            \n",
    "        if key == ord('q'):\n",
    "            \n",
    "            break\n",
    "            \n",
    "            \n",
    "    if is_stream:\n",
    "        stream.stop()\n",
    "    else:\n",
    "        video.release()\n",
    "    \n",
    "    cv2.destroyWindow('Display')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_person_tracking(path = 'demo_videos/ipcam1.mp4', is_stream = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
